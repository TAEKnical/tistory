*본 게시물은 Kubernetes Advanced Networking Study (=KANS)의 주차별 학습주제를 기반으로 작성자가 테스트한 내용을 추가하여 재가공하였습니다.  

*별도의 언급이 없는 경우실습에 사용된 Vagrantfile은 가시다님께서 공유해주신 파일을 사용하였음을 밝힙니다.
 
# 1. network namespace
컨테이너는 가상화된 공간을 생성하기 위해 pivot-root, namespace, cgroup을 사용해서 프로세스 단위로 환경과 리소스를 격리한다. 이전에는 chroot을 사용했으나, chroot는 탈옥이 가능하기 때문에 pivot-root와 mount namespace를 사용한다고 한다.

이번 주차에서 network namespace에 대한 실습을 진행하며 network namespace에 대한 개념을 익히고자 한다.


## 1-1. network device 상태 확인
```Bash
root@ubuntu-focal:~# ip -c link #link는 network device만 출력, c는 컬러 옵션

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 02:78:6c:8f:22:ad brd ff:ff:ff:ff:ff:ff
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 08:00:27:74:e6:b3 brd ff:ff:ff:ff:ff:ff
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
    link/ether 02:42:43:ff:ad:ec brd ff:ff:ff:ff:ff:ff
```
루프백 인터페이스와 docker0 인터페이스를 제외하면 enp0s3과 enps8이 있다.

```Bash
root@ubuntu-focal:~# ip -c addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:78:6c:8f:22:ad brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 86159sec preferred_lft 86159sec
    inet6 fe80::78:6cff:fe8f:22ad/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:74:e6:b3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.50.10/24 brd 192.168.50.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe74:e6b3/64 scope link
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:43:ff:ad:ec brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:43ff:feff:adec/64 scope link
       valid_lft forever preferred_lft forever
	   
root@ubuntu-focal:~# ping 8.8.8.8 -I enp0s3
PING 8.8.8.8 (8.8.8.8) from 10.0.2.15 enp0s3: 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=106 time=66.2 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=106 time=67.2 ms
64 bytes from 8.8.8.8: icmp_seq=3 ttl=106 time=65.4 ms
^C
--- 8.8.8.8 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2006ms
rtt min/avg/max/mdev = 65.443/66.252/67.155/0.701 ms

root@ubuntu-focal:~# ping 8.8.8.8 -I enp0s8
PING 8.8.8.8 (8.8.8.8) from 192.168.50.10 enp0s8: 56(84) bytes of data.
^C
--- 8.8.8.8 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2046ms

```

ip addr로 대역 확인 및 ping을 날려보면 enp0s3는 virtualbox를 실행중인 windows와 NAT방식으로, enp0s8은 host only 방식으로 연결되어 있음을 짐작할 수 있다.
[Oracle VM Virtual box network 방식 정리](https://blog.naver.com/ilikebigmac/221977237748)

```Bash
#windows의 ipconfig 출력결과

이더넷 어댑터 VirtualBox Host-Only Network #4:

   연결별 DNS 접미사. . . . :
   링크-로컬 IPv6 주소 . . . . : fe80::4870:14b0:9a4e:6793%17
   IPv4 주소 . . . . . . . . . : 192.168.50.1
   서브넷 마스크 . . . . . . . : 255.255.255.0
   기본 게이트웨이 . . . . . . :
```

![[Pasted image 20220118215512.png]] ![[Pasted image 20220118215519.png]]

실제 확인해본 결과 예상과 일치함. 기존에 존재하는 network interface에 대한 현황 파악은 마쳤으므로 가상 네트워크 인터페이스인 veth0과 veth1을 생성하여 통신을 테스트하고자 한다.


## 1-2. 네트워크 네임스페이스 간 통신(/var/run/netns)
![[Pasted image 20220118224915.png]]
*이미지 출처 : 가시다님 노션*

```Bash
root@ubuntu-focal:~# ip link add veth0 type veth peer name veth1
root@ubuntu-focal:~# ip -c addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:78:6c:8f:22:ad brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 86254sec preferred_lft 86254sec
    inet6 fe80::78:6cff:fe8f:22ad/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:74:e6:b3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.50.10/24 brd 192.168.50.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe74:e6b3/64 scope link
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:4a:11:17:fe brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether b6:52:82:44:69:8a brd ff:ff:ff:ff:ff:ff
6: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 02:98:72:34:e9:5b brd ff:ff:ff:ff:ff:ff
```

veth0과 veth1이라는 이름을 가진 network device를 veth 타입으로 새롭게 add하고, veth0과 veth1간에 peering을 맺는다.
새롭게 생성된 veth0과 veth1 인터페이스는 DOWN 상태이다.

```Bash
root@ubuntu-focal:~# ip netns add RED
root@ubuntu-focal:~# ip netns add BLUE
root@ubuntu-focal:~# ip netns list
BLUE
RED
```

RED와 BLUE 두 개의 network namespace를 추가하였다.

```Bash
root@ubuntu-focal:~# ip link set veth0 netns RED

root@ubuntu-focal:~# ip link set veth1 netns BLUE

root@ubuntu-focal:~# ip netns exec RED ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
6: veth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 02:98:72:34:e9:5b brd ff:ff:ff:ff:ff:ff link-netns BLUE
root@ubuntu-focal:~# ip netns exec BLUE ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
5: veth1@if6: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether b6:52:82:44:69:8a brd ff:ff:ff:ff:ff:ff link-netns RED

```

veth0을 network namespace RED로, veth1을 network namespace BLUE로 이동시키고 각 네트워크 네임스페이스에 대하여 ip -c a(ddr) 명령을 실행하면 실제로 veth0과 veth1이 netwrok device 목록에 추가되었음을 확인할 수 있다.

```Bash
root@ubuntu-focal:~# ip -c addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:78:6c:8f:22:ad brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 85828sec preferred_lft 85828sec
    inet6 fe80::78:6cff:fe8f:22ad/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:74:e6:b3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.50.10/24 brd 192.168.50.255 scope global enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::a00:27ff:fe74:e6b3/64 scope link
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:4a:11:17:fe brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
```

동시에 root namespace의 network device 목록에서는 사라졌다. 이를 통해 root namespace 역시 다른 network namespace들과 서로 간 격리됨을 알 수 있었다.

```Bash
root@ubuntu-focal:~# ip netns exec RED ip link set veth0 up
root@ubuntu-focal:~# ip netns exec RED ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
6: veth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 02:98:72:34:e9:5b brd ff:ff:ff:ff:ff:ff link-netns BLUE
    inet6 fe80::98:72ff:fe34:e95b/64 scope link
       valid_lft forever preferred_lft forever
root@ubuntu-focal:~# ip netns exec BLUE ip link set veth1 up
root@ubuntu-focal:~# ip netns exec BLUE ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
5: veth1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether b6:52:82:44:69:8a brd ff:ff:ff:ff:ff:ff link-netns RED
    inet6 fe80::b452:82ff:fe44:698a/64 scope link
       valid_lft forever preferred_lft forever
```

DOWN 상태에 있던 veth0과 veth1을 동작시켜 UP 상태를 확인.

```Bash
root@ubuntu-focal:~# ip netns exec RED ip addr add 11.11.11.12/24 dev veth0
root@ubuntu-focal:~# ip netns exec RED ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
6: veth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 02:98:72:34:e9:5b brd ff:ff:ff:ff:ff:ff link-netns BLUE
    inet 11.11.11.12/24 scope global veth0
       valid_lft forever preferred_lft forever
    inet6 fe80::98:72ff:fe34:e95b/64 scope link
       valid_lft forever preferred_lft forever
root@ubuntu-focal:~# ip netns exec BLUE ip addr add 11.11.11.13/24 dev veth1
root@ubuntu-focal:~# ip netns exec BLUE ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
5: veth1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether b6:52:82:44:69:8a brd ff:ff:ff:ff:ff:ff link-netns RED
    inet 11.11.11.13/24 scope global veth1
       valid_lft forever preferred_lft forever
    inet6 fe80::b452:82ff:fe44:698a/64 scope link
       valid_lft forever preferred_lft forever
```

통신 테스트를 위해 각 namespace의 network device(veth0, veth1)에 ip를 부여한다.

```Bash
root@ubuntu-focal:~# tree /var/run/netns
/var/run/netns
├── BLUE
└── RED

0 directories, 2 files

root@ubuntu-focal:/var/run/netns# ls
BLUE  RED
root@ubuntu-focal:/var/run/netns# file BLUE
BLUE: empty
```

현재 netns 상태를 /var/run/netns 를 조회하여 확인할 수 있다. 실제로 확인해보면 netns 이름으로 된 빈 파일만 존재한다. 파일을 까보고 싶었는데 비어있어서 내용을 확인할 수는 없었다.

---
(그래도 궁금해서 찾아봄..)

*/var/run/netns*

netns가 생성/제거될 때 ip command는 생성된 netns에 대한 bind mount point를 /var/run/netns 경로에 생성함으로써, 이 네임스페이스에 어떤 프로세스가 돌고 있지 않다고 하더라도 네임스페이스가 사라진다거나 하지 않게 된다.

ip netns 명령이 동작할 때 참조하는 경로가 /var/run/netns가 되는데, 가령 network namespace를 listing 할 때, 이 경로의 파일을 읽어서 network namespace 목록을 보여준다.

---

## 1-3. 네임스페이스 간 통신 확인

```Bash
##Terminal 1 / Terminal 2에서 진행
root@ubuntu-focal:/var/run/netns# nsenter --net=/var/run/netns/RED
root@ubuntu-focal:/var/run/netns# ip -c a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
6: veth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 02:98:72:34:e9:5b brd ff:ff:ff:ff:ff:ff link-netns BLUE
    inet 11.11.11.12/24 scope global veth0
       valid_lft forever preferred_lft forever
    inet6 fe80::98:72ff:fe34:e95b/64 scope link
       valid_lft forever preferred_lft forever
root@ubuntu-focal:/var/run/netns# ip -c neigh
root@ubuntu-focal:/var/run/netns# ip -c route
11.11.11.0/24 dev veth0 proto kernel scope link src 11.11.11.12
root@ubuntu-focal:/var/run/netns# iptables -t filter -S
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
root@ubuntu-focal:/var/run/netns# iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
```

터미널을 여러개 열고 RED와 BLUE 각각에 대해서 nsenter 명령으로 네임스페이스에 진입할 수 있다. root namespace와는 별개의 network resource를 갖게 됨을 확인할 수 있다.

```Bash
#host terminal
root@ubuntu-focal:~# lsns -t net
        NS TYPE NPROCS   PID USER    NETNSID NSFS            COMMAND
4026531992 net     132     1 root unassigned                 /sbin/init
4026532190 net       1  1773 root          0 /run/netns/RED  -bash
4026532252 net       1  1900 root          1 /run/netns/BLUE -bash
root@ubuntu-focal:~# ip -c neigh
10.0.2.2 dev enp0s3 lladdr 52:54:00:12:35:02 REACHABLE
root@ubuntu-focal:~# ip -c route
default via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100
10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15
10.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.50.0/24 dev enp0s8 proto kernel scope link src 192.168.50.10
```
호스트에서 확인해보면 2개의 netns가 동작중임을 확인할 수 있다.
ip -c neigh 명령으로 arp 테이블을, ip -c route 명령으로 route 테이블을 확인할 수 있다.

```Bash
# Terminal 3
root@ubuntu-focal:~# tcpdump -i veth1
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes
13:30:51.678993 ARP, Request who-has 11.11.11.13 tell 11.11.11.12, length 28
13:30:51.679006 ARP, Reply 11.11.11.13 is-at b6:52:82:44:69:8a (oui Unknown), length 28
13:30:51.679008 IP 11.11.11.12 > 11.11.11.13: ICMP echo request, id 2039, seq 1, length 64
13:30:51.679015 IP 11.11.11.13 > 11.11.11.12: ICMP echo reply, id 2039, seq 1, length 64
13:30:56.686185 ARP, Request who-has 11.11.11.12 tell 11.11.11.13, length 28
13:30:56.686230 ARP, Reply 11.11.11.12 is-at 02:98:72:34:e9:5b (oui Unknown), length 28
^C
6 packets captured
6 packets received by filter
0 packets dropped by kernel
root@ubuntu-focal:~# ip -c neigh
11.11.11.12 dev veth1 lladdr 02:98:72:34:e9:5b REACHABLE

root@ubuntu-focal:~# ip -c addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
5: veth1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether b6:52:82:44:69:8a brd ff:ff:ff:ff:ff:ff link-netns RED
    inet 11.11.11.13/24 scope global veth1
       valid_lft forever preferred_lft forever
    inet6 fe80::b452:82ff:fe44:698a/64 scope link
       valid_lft forever preferred_lft forever
```


```Bash
root@ubuntu-focal:/var/run/netns# ping 11.11.11.13 -c 1
PING 11.11.11.13 (11.11.11.13) 56(84) bytes of data.
64 bytes from 11.11.11.13: icmp_seq=1 ttl=64 time=0.033 ms

--- 11.11.11.13 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.033/0.033/0.033/0.000 ms

root@ubuntu-focal:/var/run/netns# ip -c neigh
11.11.11.13 dev veth0 lladdr b6:52:82:44:69:8a REACHABLE
```

BLUE netns에서 tcpdump로 tcp 패킷을 캡처함과 동시에, RED netns에서 BLUE netns로 ping을 날려보면 통신이 이루어지고 있음을 확인할 수 있다.
RED netns의 arp 테이블을 보면, 11.11.11.13(BLUE netns의 veth1)에 대한 mac address(b6:52:82:44:69:8a)가 확인된다.

# 2. Bridge를 이용한 netns 간 통신(iptables)
![[Pasted image 20220119220146.png]]
도커는 bridge를 이용한 network 통신을 한다. 도커의 기본 동작 환경은 bridge network가 존재하고, 이 bridge를 이용해서 컨테이너간 통신이 이루어진다. 컨테이너가 외부 통신을 하려면 briedge를 통해 iptable을 타고 외부 network interface를 거쳐 인/아웃바운드 통신이 이루어진다. 그 가운데에 bridge라는 network device를 컨테이너 없이 구현해서 동작 방식을 확인하고자 한다.  L2 스위치를 가운데 두고 컨테이너를 전부 여기에 연결해 두는 구조로 보면 될 것 같다.

환경 구성
```Bash
root@ubuntu-focal:~# ip netns add RED
root@ubuntu-focal:~# ip link add reth0 type veth peer name reth1
root@ubuntu-focal:~# ip link set reth0 netns RED
root@ubuntu-focal:~# ip netns add BLUE
root@ubuntu-focal:~# ip link add beth0 type veth peer name beth1
root@ubuntu-focal:~# ip link set beth0 netns BLUE
root@ubuntu-focal:~# ip netns list
```

bridge 생성
```Bash
root@ubuntu-focal:~# brctl show
bridge name     bridge id               STP enabled     interfaces
docker0         8000.0242f329dfa0       no
root@ubuntu-focal:~# ip link add br0 type bridge
root@ubuntu-focal:~# brctl show
bridge name     bridge id               STP enabled     interfaces
br0             8000.000000000000       no
docker0         8000.0242f329dfa0       no
```
가상의 bridge(L2 스위치)가 추가됨.

reth0(RED)-reth1(host), beth0(BLUE)-beth1(host) , bridge(host) 가 준비되었으므로 reth1과 beth1을 bridge에 연결해야 한다.
```Bash
root@ubuntu-focal:~# ip link set reth1 master br0
root@ubuntu-focal:~# ip link set beth1 master br0
root@ubuntu-focal:~# brctl show br0
bridge name     bridge id               STP enabled     interfaces
br0             8000.4e6f0c46c58d       no              beth1
                                                        reth1
```

reth0과 beth0에 IP를 부여하고, 모든 인터페이스를 활성화한다. 그러나 RED에서 BLUE로 ping을 해보면 실패함.
```Bash
root@ubuntu-focal:~# ip netns exec RED ip addr add 11.11.11.12/24 dev reth0
root@ubuntu-focal:~# ip netns exec BLUE ip addr add 11.11.11.13/24 dev beth0
root@ubuntu-focal:~# ip netns exec RED ip link set reth0 up; ip link set reth1 up
root@ubuntu-focal:~# ip netns exec BLUE ip link set beth0 up; ip link set beth1 up
root@ubuntu-focal:~# ip link set br0 up
root@ubuntu-focal:~# ip -br -c addr
lo               UNKNOWN        127.0.0.1/8 ::1/128
enp0s3           UP             10.0.2.15/24 fe80::78:6cff:fe8f:22ad/64
enp0s8           UP             192.168.50.10/24 fe80::a00:27ff:fe74:e6b3/64
docker0          DOWN           172.17.0.1/16
reth1@if6        UP             fe80::9441:5aff:fe8e:ff8d/64
beth1@if8        UP             fe80::4c6f:cff:fe46:c58d/64
br0              UP             fe80::4c6f:cff:fe46:c58d/64
```


```Bash
root@ubuntu-focal:~# iptables -t filter -S
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION-STAGE-1
-N DOCKER-ISOLATION-STAGE-2
-N DOCKER-USER
-A FORWARD -j DOCKER-USER
-A FORWARD -j DOCKER-ISOLATION-STAGE-1
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
-A DOCKER-ISOLATION-STAGE-1 -j RETURN
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
```

```Bash
root@ubuntu-focal:~# tcpdump -l -i br0
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes
14:21:02.256604 ARP, Request who-has 11.11.11.13 tell 11.11.11.12, length 28
14:21:02.256633 ARP, Reply 11.11.11.13 is-at f2:4c:9c:12:cc:34 (oui Unknown), length 28
14:21:02.256637 IP 11.11.11.12 > 11.11.11.13: ICMP echo request, id 2487, seq 1, length 64
```
br0는 host 네임스페이스의 인터페이스이므로 host의 iptables 영향을 받는다. host iptables의 FOWARD 옵션 값이 DROP으로 되어있으므로, 포워딩이 필요한 패킷을 드랍하게 된다.

단, BLUE쪽에서 RED가 보낸 ping은 확인되지 않더라도 ARP는 Request와 Reply가 모두 확인된다. iptables는 Layer3에서 동작하기 때문.

**iptables**
iptables에는 정책만 세우게 되고, 그 정책이 실제로 적용되어서 차단/허용이 수행되는 것은 kernel 레벨의 Netfilter 프레임워크에 의해 이루어진다. 이 Netfilter 프레임워크를 백엔드에 두고 사용 가능한 어플리케이션 중 하나가 iptables이다. 만약 kernel 레벨이 아닌 user레벨에서 이 동작이 이루어지면 훨씬 느려졌을 것.
![[Pasted image 20220119233817.png]]
![[Pasted image 20220119233336.png]]

```bash
root@ubuntu-focal:~# iptables -t filter -I DOCKER-USER -j ACCEPT

root@ubuntu-focal:~# tcpdump -l -i br0
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on br0, link-type EN10MB (Ethernet), capture size 262144 bytes
14:44:30.033312 IP 11.11.11.12 > 11.11.11.13: ICMP echo request, id 2498, seq 1, length 64
14:44:30.033343 IP 11.11.11.13 > 11.11.11.12: ICMP echo reply, id 2498, seq 1, length 64

```

netns(또는 컨테이너) 간에 통신이 이루어 질 때는 br0를 가지고 있는 host 입장에서 이들을 다른 시스템으로 인지한다. 따라서  Prerouting->Forward->Postrouting 룰을 타게 되고, 목적지가 host인 경우는 위 그림 기준으로 INPUT을 타고 들어가게 된다. Forward 룰이 DROP 되어있는 것을 ACCEPT로 바꿔 주어야 함.




 
>실습3
![[Pasted image 20220120214811.png]]

반대로 host에서 컨테이너 내부로 통신이 가능하려면 어떻게 해야 할까? br0도 IP를 부여받아야 한다. kubernetes의 모든 CNI도 이와 같은 인터페이스를 가지고 통신을 하게 된다.

```Bash
ip addr add 11.11.11.11/24 dev br0
```


한 단계 더 나아가서, 위와 같은 환경에서 RED 또는 BLUE(컨테이너로 지칭) 외부 통신을 하려면 어떻게 해야 할까? 8.8.8.8로의 통신을 테스트한다고 가정하면 우선 컨테이너 안의 routing table에 8.8.8.8에 대한 규칙이 정의되어 있지 않다. 

```Bash
ip -c route
ip route add default via 11.11.11.11
```

RED나 BLUE의 라우팅 명령을 확인해보면 서로간의 경로만 가지고 있다. 여기에 규칙을 추가
해서 default 라우팅 규칙으로 11.11.11.11(br0)을 정의하면, 다른 규칙에 해당하지 않는 트래픽은 모두 host로 가게 된다.


```Bash

```
컨테이너 내부에서 8.8.8.8을 쳤을 때 라우팅 규칙에 따라 host로 넘어감을 tcp dump로 확인할 수 있다. 컨테이너->host로의 통신은 확인되었으니, host->외부로 통신이 되지 않는 이유를 생각해보면 이 역시 iptables가 관여하기 때문이다.

```Bash
iptables -t nat -S
...
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
...
```
NAT 통신에 대한 iptables 규칙을 보면, docker에서 올라간 컨테이너는 172.17.0.0/16 대역의 IP를 할당받고, 외부로 빠져나갈 때 MASQUERADE 되어 공인 IP로 나가도록 정의되어 있으나 우리가 정의한 br0는 이 규칙을 정의하지 않았다.

```Bash
iptables -t nat -A POSTROUTING -s 11.11.11.0/24 -j MASQUERADE

contrack -L --src-nat
```
출발지가 11.11.11.0/24인 대역도 MASQUERADE를 할 수 있도록 정의하면 외부 통신이 가능해짐을 확인할 수 있다.

추가로 NAT를 사용하면 응답을 받을 떄 공인IP 하나로만 받기 때문에 뒷 단에 있는 어떤 시스템에서 요청을 건네받을 것인지 확인이 필요하다. 이럴 떄 NAT 테이블 정보에서 어느 시스템에서 요청을 내보냈고, 받아야 하는지에 대한 내용이 필요하고, 이 내용을 볼 수 있는 명령이 contrack이다.



![[Pasted image 20220122100850.png]]
![[Pasted image 20220122100858.png]]
docker network 방식에는 여러가지가 있지만 지금까지의 실습 과정은 docker를 설치하고 컨테이너를 만들 때의 과정에서 docker network의 bridege 환경을 구현한 것과 같다. docker를 이용해서 컨테이너를 생성하면 컨테이너는 172.17.0.0/16 대역의 IP를 할당받고 컨테이너의 veth는 docker0 briedge에 연결되며, iptables의 규칙들이 정의된다. 




# References
[network-namespaces] https://blogs.igalia.com/dpino/2016/04/10/network-namespaces/
[/var/run/netns](http://manpages.ubuntu.com/manpages/trusty/man8/ip-netns.8.html)
[iptables](https://www.thegeekstuff.com/2011/01/iptables-fundamentals/)
[docker network 방식] https://docs.docker.com/network/
[Oracle VM Virtual box network 방식 정리](https://blog.naver.com/ilikebigmac/221977237748)